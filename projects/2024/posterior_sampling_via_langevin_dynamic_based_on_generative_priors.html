<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Consistency Posterior Sampling for Diverse Image Synthesis">
  <meta name="keywords" content="Posterior sampling, Consistency Models, Generative Priors, Bayesian Imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Consistency Posterior Sampling for Diverse Image Synthesis</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../2023/static/css/bulma.min.css">
  <link rel="stylesheet" href="../2023/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../2023/static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="../2023/static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../2023/static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../2023/static/js/fontawesome.all.min.js"></script>
  <script src="../2023/static/js/bulma-carousel.min.js"></script>
  <script src="../2023/static/js/bulma-slider.min.js"></script>
  <!-- <script src="../2023/static/js/index.js"></script>
  <script src="../2023/static/js/gallery.js"></script> -->
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://vishal-s-p.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Consistency Posterior Sampling for Diverse Image Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="../../index.html" target="_blank">Vishal Purohit<sup>♦,</sup>*, </a>
              </span>
              <span class="author-block">
                <a href="https://mrepasky3.github.io/" target="_blank">Matthew Repasky<sup>‡,</sup>*, </a>
              </span>
              <span class="author-block">
                <a href="https://services.math.duke.edu/~jianfeng/" target="_blank">Jianfeng Lu<sup>†,△</sup>, </a>
              </span>
              <span class="author-block"></span>
                <a href="https://web.ics.purdue.edu/~qqiu/" target="_blank">Qiang Qiu<sup>♦</sup>, </a>
              </span>
              <span class="author-block">
                <a href="https://www2.isye.gatech.edu/~yxie77/" target="_blank">Yao Xie<sup>‡</sup>, </a>
              </span>
              <span class="author-block"></span>
                <a href="https://services.math.duke.edu/~xiuyuanc/" target="_blank">Xiuyuan Cheng<sup>†</sup></a>
              </span>
          </div>

          <div class="is-size-5 publication-authors">
            <p><span class="author-block">♦ Elmore Family School of Electrical and Computer Engineering, Purdue University, USA</span></p>
            <p><span class="author-block">† Department of Mathematics, Duke University, USA</span></p>
            <p><span class="author-block">△ Department of Physics and Department of Chemistry, Duke University, USA</span></p>
            <p><span class="author-block">‡ H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, USA</span></p>
            <p><span class="author-block">* Equal Contribution</span></p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.02078"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (Coming Soon!)</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Vishal-S-P/CPS_Diverse"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/GQCI_teaser.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified cardabstract">
          Posterior sampling in high-dimensional spaces using generative models 
          holds significant promise for various applications, including but not limited to inverse problems and
          guided generation tasks. Generating diverse posterior samples remains expensive, as existing methods require restart-
          ing the entire generative process for each new sample. In this
          work, we propose a posterior sampling approach that simulates Langevin dynamics in the noise space of a pre-trained
          generative model. By exploiting the mapping between the
          noise and data spaces which can be provided by distilled
          flows or consistency models, our method enables seamless
          exploration of the posterior without the need to re-run the
          full sampling chain, drastically reducing computational overhead. Theoretically, we prove a guarantee for the proposed
          noise-space Langevin dynamics to approximate the posterior,
          assuming that the generative model sufficiently approximates
          the prior distribution. Our framework is experimentally validated on image restoration tasks involving noisy linear and
          nonlinear forward operators applied to LSUN-Bedroom (256
          × 256) and ImageNet (64 × 64) datasets. The results demonstrate that our approach generates high-fidelity samples with
          enhanced semantic diversity even under a limited number
          of function evaluations, offering superior efficiency and performance compared to existing diffusion-based posterior
          sampling techniques.
      </div>
    </div>
  </div>
</section>
<!-- <section class="section">
    <div class="container is-max-desktop">
    <div class="image-caption-container">
      <h2 class="title is-3">Colorization Challenge under Varying Exposure Conditions</h2>
      <img src="./static/images/exp_synth_proj_1.png" alt="Descriptive text about the image">
      <div class="im_caption">
        <figcaption>Illustration of exposure correction and colorization of binary images using neural networks. (a) depicts a range of images from overexposed to underexposed, illustrating the degradation of image details due to exposure variation. (b) contrasts the standard colorization workflow and our proposed approach. (i) In standard colorization approaches, a neural network learns to map a binary image \(\mathbf{Y}\) to the corresponding color image \(\mathbf{X}_{c}\) via a neural network, \(\mathcal{F}^{\text{aug}}\), where superscript `aug' indicates the colorizer is trained using dataset with augmented exposure images.  (ii) In contrast, our approach does not require training colorizer with augmented exposure images. (c) compares the colorization results: the first row is the output of a colorizer trained without augmentation, the second row is the output of colorizer trained with augmented data, the third row corresponds to the results of our method and the last provides the ground truth images for reference.</figcaption>
      <!-- <figcaption>Illustration of exposure correction and colorization of binary images using neural networks. (a) depicts a range of images from overexposed to underexposed, illustrating the degradation of image details due to exposure variation. (b) contrasts the standard colorization workflow and our proposed approach. In standard colorization approaches, a neural network learns to map a binary image to the corresponding color image via a neural network, \(\mathcal{F}^{aug}\), where superscript \(\textit{aug}\) indicates the colorizer is trained using datasets of augmented exposure images. Our approach does not require training colorizer with augmented exposure images. (c) compares the colorization results: the first row is the output of a colorizer trained without augmentation, the second row is the output of colorizer trained with augmented data, the third row corresponds to the results of our method and the last provides the ground truth images for reference.</figcaption> -->
      </div>
    </div>
  </div>
</section> -->
<!-- <section class="section">
    <div class="container is-max-desktop">
    <div class="image-caption-container">
      <h2 class="title is-3">Methodology</h2>
      <img src="./static/images/our_method_proj_1.png" alt="Descriptive text about the image">
      <div class="im_caption">
      <figcaption>An illustration of our proposed method for exposure adaptive colorization: A binary image, \(\mathbf{Y}\), which can be overexposed or underexposed, is input into the proposed exposure synthesis module. The colorization of binary image can be achieved using Single Image Colorization (SIC) or Burst Image Colorization (BIC). (i) <b>SIC:</b> Based on the input and target exposure levels, \(\widetilde\theta_\text{input}\) and \(\widetilde\theta_\text{target}\), this module adjusts the weights of a exposure synthesis network \(\mathcal{G}\), which then generates an exposure-corrected image. Note that corrected image is not necessarily a binary image. Since the colorization module \(\mathcal{F}\) is trained to colorize only image of specific exposure, the exposure synthesis module ensures corrected binary image has similar exposure to the one on which \(\mathcal{F}\) is trained. (\romannumeral 2) <b>BIC:</b> For BIC we generate images with varying exposures as input to the burst image colorization network, \(\mathcal{F}_{\text{burst}}\). The trained network is able to exploit the complementary information across multiple exposures with the help of Cross Non-Local Fusion blocks to synthesize colors in regions of the image that is otherwise not possible by SIC approach.</figcaption>
      
    </div>
    </div>
</section> -->




<!-- Paper video. -->
<!-- <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Video</h2>
    <div class="publication-video">
      <iframe src="" 
              frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
  </div>
</div> -->
<!--/ Paper video. -->
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{purohit_repasky2024,
      author = {Vishal Purohit, Matthew Repasky, Jianfeng Lu, Qiang Qiu, Yao Xie, Xiuyuan Cheng},
      title  = {Consistency Posterior Sampling for Diverse Image Synthesis},
      year   = {2025},
      booktitle = {CVPR},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Templete modified from <a
              href="https://github.com/nerfies/nerfies.github.io">source code.</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>